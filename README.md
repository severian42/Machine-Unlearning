# Unlearning For Machines and AI
A Specialized Exploration for Privacy and Security 

<img src="an-image-of-an-ai-model-deleting-the-data-from-its.png" alt="Unlearning Graphic" width="400" height="400">


## Table of Contents
1. [Introduction](#introduction)
2. [Scope and Objectives](#scope-and-objectives)
3. [Unlearning Methods](#unlearning-methods)
4. [Methodology](#methodology)
5. [Findings and Metrics](#findings-and-metrics)
6. [Next Steps](#next-steps)
7. [Contact](#contact)

## Introduction
This repository is devoted to the specialized area of unlearning in machine learning. While machine learning models are traditionally designed to remember, the emerging field of unlearning explores how these models can forget. Here, we investigate algorithmic methods for unlearning, with an emphasis on privacy protection and defense against inference attacks.

<a href="https://colab.research.google.com/drive/1ZitrQ92NQnvZVw0HykFSCv3VSaJ6q4pz?usp=sharing" target="_blank">Try out the Unlearning notebook here</a>

*DISCLAIMER:* These notebooks have been modified and adpated from this original source which was a Google Competition that never started. I ended up adapting the notebook so I could open source a working version of the algorithm for the community and we could keep exploring its potential. Here is the original notebook https://github.com/unlearning-challenge/starting-kit


------------------

## Sibling Repository: Biomimicry in Machine Learning

This repository is a sibling to our [Biomimicry in Machine Learning repository](https://github.com/severian42/Biomimicry-Machine-Learning). While the Biomimicry repository focuses on general algorithmic innovations inspired by nature, this Unlearning repository narrows down on specialized aspects of machine learningâ€”specifically unlearning, privacy protection, and defense against inference attacks.

--------------------

## Scope and Objectives
The repository aims to:
- Develop and test unlearning algorithms that serve real-world needs.
- Build privacy-preserving machine learning models.
- Offer empirical evidence supporting the methods' efficiency and effectiveness.

## Unlearning Methods
We explore multiple avenues for unlearning, including but not limited to:

- **Data Exclusion Techniques**: Methods for efficiently and securely removing specific data points from the model's knowledge.
- **Model Reversibility**: Techniques that ensure a model can revert to a former state, thereby erasing newly incorporated data, enhancing data security.
- **Dynamic Retraining**: Algorithms that adaptively retrain the model to forget specific subsets of data without a significant performance drop.

## Methodology
The research process consists of several iterative stages:

1. **Literature Review**: A comprehensive review of existing work in machine unlearning and privacy preservation.
2. **Algorithm Design**: Development of new algorithms or modification of existing ones to incorporate unlearning capabilities.
3. **Empirical Testing**: Testing these algorithms using predefined metrics to gauge their performance and security.
4. **Iterative Refinement**: Continuous refinement based on the empirical findings, community feedback, and emerging needs in data privacy.

## Findings and Metrics
Our evaluation criteria are multidimensional:

- **Unlearning Efficacy**: Metrics to evaluate the quality of unlearning, such as the degree of data forgotten and how it impacts overall model performance.
- **Performance Overheads**: Computational costs associated with unlearning and retraining models, balanced against the privacy benefits.
- **Robustness against Attacks**: Metrics evaluating the model's defense mechanisms against various types of inference and re-identification attacks.

## Next Steps
Future directions include:

- **Community Engagement**: Inviting open-source contributions to improve the algorithms and methodologies.
- **Validation on Larger Datasets**: Extending the validation phase to larger, more complex datasets to test scalability.
- **Further Research and Publications**: Peer-reviewed publications are in the pipeline, emphasizing empirical findings and the nuances of privacy and security in machine unlearning.

---

For inquiries or collaborations, feel free to [Email me](mailto:beckettdillon42@gmail.com) or [@Severian.Makes.Noise](https://twitter.com/SeverianMakesNoise)

